{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 7240120,
     "sourceType": "datasetVersion",
     "datasetId": 4193312
    },
    {
     "sourceId": 7267920,
     "sourceType": "datasetVersion",
     "datasetId": 4212958
    }
   ],
   "dockerImageVersionId": 30626,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType,StructField, StringType,IntegerType,BooleanType,DoubleType\n",
    "from pyspark import SparkContext, HiveContext,SQLContext"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-23T22:16:11.583245Z",
     "iopub.execute_input": "2023-12-23T22:16:11.583627Z",
     "iopub.status.idle": "2023-12-23T22:16:11.592396Z",
     "shell.execute_reply.started": "2023-12-23T22:16:11.583591Z",
     "shell.execute_reply": "2023-12-23T22:16:11.590816Z"
    },
    "trusted": true
   },
   "execution_count": 100,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-23T22:16:11.594571Z",
     "iopub.execute_input": "2023-12-23T22:16:11.595216Z",
     "iopub.status.idle": "2023-12-23T22:16:11.610011Z",
     "shell.execute_reply.started": "2023-12-23T22:16:11.595166Z",
     "shell.execute_reply": "2023-12-23T22:16:11.608577Z"
    },
    "trusted": true
   },
   "execution_count": 101,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "path = \"../data/json_IMDB/\"\n",
    "df = spark.read.option(\"multiline\",\"true\").json(path + \"*.json\")\n",
    "df = df.withColumn(\"file_name\", F.input_file_name())"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-23T22:16:11.618599Z",
     "iopub.execute_input": "2023-12-23T22:16:11.619030Z",
     "iopub.status.idle": "2023-12-23T22:16:14.099227Z",
     "shell.execute_reply.started": "2023-12-23T22:16:11.618971Z",
     "shell.execute_reply": "2023-12-23T22:16:14.096128Z"
    },
    "trusted": true
   },
   "execution_count": 102,
   "outputs": [
    {
     "name": "stderr",
     "text": "                                                                                \r",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### On commence par transposer le dataframe et changer le nom de chaque colonne par le nom du film. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_file_name(cell_value):\n",
    "    if isinstance(cell_value, str): \n",
    "        match = re.search(r'/([^/]+)\\.json$', cell_value)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "    return cell_value\n",
    "\n",
    "df_pandas = df.toPandas()\n",
    "df_pandas_T = df_pandas.T\n",
    "df_pandas_T.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for col_name in df_pandas_T.columns:\n",
    "    df_pandas_T[col_name] = df_pandas_T[col_name].apply(extract_file_name)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-23T22:16:14.100514Z",
     "iopub.execute_input": "2023-12-23T22:16:14.100941Z",
     "iopub.status.idle": "2023-12-23T22:16:15.200452Z",
     "shell.execute_reply.started": "2023-12-23T22:16:14.100902Z",
     "shell.execute_reply": "2023-12-23T22:16:15.198839Z"
    },
    "trusted": true
   },
   "execution_count": 103,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "last_values = df_pandas_T.apply(lambda col: col.iloc[-1])\n",
    "df_pandas_T.columns = last_values"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-23T22:16:15.202201Z",
     "iopub.execute_input": "2023-12-23T22:16:15.202602Z",
     "iopub.status.idle": "2023-12-23T22:16:15.212913Z",
     "shell.execute_reply.started": "2023-12-23T22:16:15.202566Z",
     "shell.execute_reply": "2023-12-23T22:16:15.211602Z"
    },
    "trusted": true
   },
   "execution_count": 104,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Maintenant, on passe du format pandas au format spark"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "spark_df = spark.createDataFrame(df_pandas_T)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-23T22:16:15.214759Z",
     "iopub.execute_input": "2023-12-23T22:16:15.215266Z",
     "iopub.status.idle": "2023-12-23T22:16:15.442379Z",
     "shell.execute_reply.started": "2023-12-23T22:16:15.215211Z",
     "shell.execute_reply": "2023-12-23T22:16:15.441083Z"
    },
    "trusted": true
   },
   "execution_count": 105,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def modify_columns_name(df, actual_name, desired_name):\n",
    "    for old_name in spark_df.columns:\n",
    "        new_name = old_name.replace(actual_name, desired_name)\n",
    "        df = df.withColumnRenamed(old_name, new_name)\n",
    "    \n",
    "    df = df.limit(df.count() - 1)\n",
    "    return df\n",
    "\n",
    "spark_df = modify_columns_name(spark_df, \"%20\",\"_\")\n",
    "spark_df = modify_columns_name(spark_df, \"commentaires_\",\"\")\n",
    "spark_df = modify_columns_name(spark_df, \".\",\"\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-23T22:16:15.444811Z",
     "iopub.execute_input": "2023-12-23T22:16:15.445547Z",
     "iopub.status.idle": "2023-12-23T22:16:32.153424Z",
     "shell.execute_reply.started": "2023-12-23T22:16:15.445499Z",
     "shell.execute_reply": "2023-12-23T22:16:32.152444Z"
    },
    "trusted": true
   },
   "execution_count": 106,
   "outputs": [
    {
     "name": "stderr",
     "text": "23/12/23 22:16:17 WARN TaskSetManager: Stage 43 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n23/12/23 22:16:23 WARN TaskSetManager: Stage 46 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n23/12/23 22:16:31 WARN TaskSetManager: Stage 49 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### On supprime les colonnes n'ayant que des valeurs nulles"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def drop_null_columns(df): \n",
    "\n",
    "    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias( \n",
    "        c) for c in df.columns]).collect()[0].asDict()\n",
    "    col_to_drop = [k for k, v in null_counts.items() if v > 0]\n",
    "    df = df.drop(*col_to_drop)\n",
    "\n",
    "    return df\n",
    "\n",
    "spark_df = drop_null_columns(spark_df)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-23T22:16:32.154697Z",
     "iopub.execute_input": "2023-12-23T22:16:32.155154Z",
     "iopub.status.idle": "2023-12-23T22:16:34.434360Z",
     "shell.execute_reply.started": "2023-12-23T22:16:32.155111Z",
     "shell.execute_reply": "2023-12-23T22:16:34.433407Z"
    },
    "trusted": true
   },
   "execution_count": 107,
   "outputs": [
    {
     "name": "stderr",
     "text": "23/12/23 22:16:33 WARN TaskSetManager: Stage 52 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Maintenant on sauvegarde les donn√©es au format parquet"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "spark_df.write.mode(\"overwrite\").parquet(\"avis_films_imd\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-12-23T22:16:34.437495Z",
     "iopub.execute_input": "2023-12-23T22:16:34.437867Z",
     "iopub.status.idle": "2023-12-23T22:16:35.740681Z",
     "shell.execute_reply.started": "2023-12-23T22:16:34.437834Z",
     "shell.execute_reply": "2023-12-23T22:16:35.739489Z"
    },
    "trusted": true
   },
   "execution_count": 108,
   "outputs": [
    {
     "name": "stderr",
     "text": "23/12/23 22:16:34 WARN TaskSetManager: Stage 55 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r",
     "output_type": "stream"
    }
   ]
  }
 ]
}
