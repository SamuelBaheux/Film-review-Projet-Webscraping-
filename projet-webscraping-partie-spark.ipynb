{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7240120,"sourceType":"datasetVersion","datasetId":4193312},{"sourceId":7267920,"sourceType":"datasetVersion","datasetId":4212958}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pyspark\n!pip install findspark","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:15:42.505667Z","iopub.execute_input":"2023-12-23T22:15:42.506184Z","iopub.status.idle":"2023-12-23T22:16:11.581087Z","shell.execute_reply.started":"2023-12-23T22:15:42.506142Z","shell.execute_reply":"2023-12-23T22:16:11.579733Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: pyspark in /opt/conda/lib/python3.10/site-packages (3.5.0)\nRequirement already satisfied: py4j==0.10.9.7 in /opt/conda/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: findspark in /opt/conda/lib/python3.10/site-packages (2.0.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType,StructField, StringType, \\\nIntegerType,BooleanType,DoubleType\nfrom pyspark import SparkContext, HiveContext,SQLContext","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:16:11.583245Z","iopub.execute_input":"2023-12-23T22:16:11.583627Z","iopub.status.idle":"2023-12-23T22:16:11.592396Z","shell.execute_reply.started":"2023-12-23T22:16:11.583591Z","shell.execute_reply":"2023-12-23T22:16:11.590816Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"spark = SparkSession.builder.getOrCreate()","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:16:11.594571Z","iopub.execute_input":"2023-12-23T22:16:11.595216Z","iopub.status.idle":"2023-12-23T22:16:11.610011Z","shell.execute_reply.started":"2023-12-23T22:16:11.595166Z","shell.execute_reply":"2023-12-23T22:16:11.608577Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/movies-datas/\"\ndf = spark.read.option(\"multiline\",\"true\").json(path + \"*.json\")\ndf = df.withColumn(\"file_name\", F.input_file_name())","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:16:11.618599Z","iopub.execute_input":"2023-12-23T22:16:11.619030Z","iopub.status.idle":"2023-12-23T22:16:14.099227Z","shell.execute_reply.started":"2023-12-23T22:16:11.618971Z","shell.execute_reply":"2023-12-23T22:16:14.096128Z"},"trusted":true},"execution_count":102,"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"We will transpose the dataframe and change the name of each column by the name of the movie","metadata":{}},{"cell_type":"code","source":"def extract_file_name(cell_value):\n    if isinstance(cell_value, str): \n        match = re.search(r'/([^/]+)\\.json$', cell_value)\n        if match:\n            return match.group(1)\n    return cell_value\n\ndf_pandas = df.toPandas()\ndf_pandas_T = df_pandas.T\ndf_pandas_T.reset_index(drop=True, inplace=True)\n\nfor col_name in df_pandas_T.columns:\n    df_pandas_T[col_name] = df_pandas_T[col_name].apply(extract_file_name)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:16:14.100514Z","iopub.execute_input":"2023-12-23T22:16:14.100941Z","iopub.status.idle":"2023-12-23T22:16:15.200452Z","shell.execute_reply.started":"2023-12-23T22:16:14.100902Z","shell.execute_reply":"2023-12-23T22:16:15.198839Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"code","source":"last_values = df_pandas_T.apply(lambda col: col.iloc[-1])\ndfP_T.columns = last_values","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:16:15.202201Z","iopub.execute_input":"2023-12-23T22:16:15.202602Z","iopub.status.idle":"2023-12-23T22:16:15.212913Z","shell.execute_reply.started":"2023-12-23T22:16:15.202566Z","shell.execute_reply":"2023-12-23T22:16:15.211602Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"now, we are switching from the pandas format to Spark","metadata":{}},{"cell_type":"code","source":"spark_df = spark.createDataFrame(df_pandas_T)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:16:15.214759Z","iopub.execute_input":"2023-12-23T22:16:15.215266Z","iopub.status.idle":"2023-12-23T22:16:15.442379Z","shell.execute_reply.started":"2023-12-23T22:16:15.215211Z","shell.execute_reply":"2023-12-23T22:16:15.441083Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"def modify_columns_name(df, actual_name, desired_name):\n    for old_name in spark_df.columns:\n        new_name = old_name.replace(actual_name, desired_name)\n        df = df.withColumnRenamed(old_name, new_name)\n    \n    df = df.limit(df.count() - 1)\n    return df\n\nspark_df = modify_columns_name(spark_df, \"%20\",\"_\")\nspark_df = modify_columns_name(spark_df, \"commentaires_\",\"\")\nspark_df = modify_columns_name(spark_df, \".\",\"\")","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:16:15.444811Z","iopub.execute_input":"2023-12-23T22:16:15.445547Z","iopub.status.idle":"2023-12-23T22:16:32.153424Z","shell.execute_reply.started":"2023-12-23T22:16:15.445499Z","shell.execute_reply":"2023-12-23T22:16:32.152444Z"},"trusted":true},"execution_count":106,"outputs":[{"name":"stderr","text":"23/12/23 22:16:17 WARN TaskSetManager: Stage 43 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n23/12/23 22:16:23 WARN TaskSetManager: Stage 46 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n23/12/23 22:16:31 WARN TaskSetManager: Stage 49 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We will drop the column with only null values","metadata":{}},{"cell_type":"code","source":"def drop_null_columns(df): \n\n    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias( \n        c) for c in df.columns]).collect()[0].asDict()\n    col_to_drop = [k for k, v in null_counts.items() if v > 0]\n    df = df.drop(*col_to_drop)\n\n    return df\n\nspark_df = drop_null_columns(spark_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:16:32.154697Z","iopub.execute_input":"2023-12-23T22:16:32.155154Z","iopub.status.idle":"2023-12-23T22:16:34.434360Z","shell.execute_reply.started":"2023-12-23T22:16:32.155111Z","shell.execute_reply":"2023-12-23T22:16:34.433407Z"},"trusted":true},"execution_count":107,"outputs":[{"name":"stderr","text":"23/12/23 22:16:33 WARN TaskSetManager: Stage 52 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r","output_type":"stream"}]},{"cell_type":"markdown","source":"Now, we save the datas on a parquet file","metadata":{}},{"cell_type":"code","source":"spark_df.write.mode(\"overwrite\").parquet(\"avis_films_imd\")","metadata":{"execution":{"iopub.status.busy":"2023-12-23T22:16:34.437495Z","iopub.execute_input":"2023-12-23T22:16:34.437867Z","iopub.status.idle":"2023-12-23T22:16:35.740681Z","shell.execute_reply.started":"2023-12-23T22:16:34.437834Z","shell.execute_reply":"2023-12-23T22:16:35.739489Z"},"trusted":true},"execution_count":108,"outputs":[{"name":"stderr","text":"23/12/23 22:16:34 WARN TaskSetManager: Stage 55 contains a task of very large size (3451 KiB). The maximum recommended task size is 1000 KiB.\n                                                                                \r","output_type":"stream"}]}]}